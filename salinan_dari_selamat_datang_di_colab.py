# -*- coding: utf-8 -*-
"""Salinan dari Selamat Datang di Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i5SV5_nwykGT60LOvqTdPls7eLm5ugU2
"""

!pip install langdetect
!pip install google-play-scraper
!pip install nltk
!pip install seaborn
!pip install pyspellchecker
!pip install symspellpy
!pip install drive
!pip install tqdm

from google_play_scraper import reviews, Sort
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk import pos_tag
from langdetect import detect, DetectorFactory
import seaborn as sns
import matplotlib.pyplot as plt
from symspellpy.symspellpy import SymSpell, Verbosity
from tqdm.notebook import tqdm
import torch


nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger_eng')

app_id = 'com.zhiliaoapp.musically'
result, _ = reviews(
    app_id,
    lang='en',
    country='gb',
    sort=Sort.NEWEST,
    count=6000,
    filter_score_with=None)

df = pd.DataFrame(result)
print(df[['userName', 'score', 'content', 'at']])
df.to_csv('tiktok_reviews.csv', index=False)

file_name = 'tiktok_reviews.csv'
df = pd.read_csv(file_name)
df

df['at'] = pd.to_datetime(df['at'])
df['hour'] = df['at'].dt.hour
hourly_reviews = df.groupby('hour')['at'].count()
plt.figure(figsize=(13, 6))
plt.plot(hourly_reviews.index, hourly_reviews.values)
plt.xlabel('Jam')
plt.ylabel('Rata-rata banyak review')
plt.title('Jumlah komen berdasarkan waktu')
plt.xticks(range(24), [f'{i:02d}.00' for i in range(24)])
plt.grid(False)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

ax = df['score'].value_counts().sort_index().plot(
    kind='bar',
    title='Ulasan Tiktok berdasarkan bintang',
    figsize=(10,5)
)
ax.set_xlabel('Bintang')
ax.set_ylabel('Jumlah')
for p in ax.patches:
    ax.text(
        p.get_x() + p.get_width() / 2,  # posisi x (tengah bar)
        p.get_height() + 1,             # posisi y (di atas bar)
        int(p.get_height()),            # nilai yang ditampilkan
        ha='center'                     # horizontal alignment
    )

plt.show()

# fungsi mendeteksi teks inggris
def is_english(text):
    try:
        return detect(text) == 'en'
    except:
        return False
# untuk menerapkan fungsi
df['is_english'] = df['content'].apply(is_english)
language_counts = df['is_english'].value_counts().rename({True: 'English', False: 'Non-English'})
#  untuk column content yang berbahasa inggris tetap ada
df = df[df['is_english'] == True].drop(columns='is_english')
# untuk menghapus index column content yang tidak berbahasa inggris
df.reset_index(drop=True, inplace=True)
df

plt.figure(figsize=(6, 4))
sns.barplot(x=language_counts.index, y=language_counts.values, palette='pastel')
plt.title('Jumlah Konten Bahasa: English vs Non-English')
plt.ylabel('Jumlah Konten')
plt.xlabel('Bahasa')
plt.tight_layout()
plt.show()

ax = df['score'].value_counts().sort_index().plot(
    kind='bar',
    title='Ulasan Tiktok berdasarkan bintang',
    figsize=(10,5)
)
ax.set_xlabel('Bintang')
ax.set_ylabel('Jumlah')
for p in ax.patches:
    ax.text(
        p.get_x() + p.get_width() / 2,  # posisi x (tengah bar)
        p.get_height() + 1,             # posisi y (di atas bar)
        int(p.get_height()),            # nilai yang ditampilkan
        ha='center'                     # horizontal alignment
    )
plt.show()

"""**Pre-Processing**

syspellpy merupakan sebuah library yang memerlukan Dictionary jauh lebih cepat dan efisien untuk mengoreksi ejaan.
link download dictionary :
https://github.com/wolfgarbe/SymSpell/raw/master/SymSpell/frequency_dictionary_en_82_765.txt
"""

# Setup
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()
sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)

# Load dictionary
dictionary_path = 'frequency_dictionary_en_82_765.txt'
sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)

def get_wordnet_pos(word):
    tag = pos_tag([word])[0][1][0].upper()
    tag_dict = {'J': wordnet.ADJ,
                'N': wordnet.NOUN,
                'V': wordnet.VERB,
                'R': wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)

"""**Cleaning Text**"""

def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)
    text = re.sub(r'\@\w+|\#', '', text)
    text = re.sub(r'[^a-z\s]', '', text)
    return text
df['cleaned_content'] = df['content'].apply(clean_text)
dt = df[['content','cleaned_content','score']]
dt['cleaned_content'].head(12)

"""**Tokenisasi Teks**"""

def tokenize_text(text):
    return word_tokenize(text)
dt['cleaned_content'] = dt['cleaned_content'].apply(tokenize_text)
dt['cleaned_content'].head(12)

"""**Stopword Remover**"""

def remove_stopwords(tokens):
    return [w for w in tokens if w not in stop_words]
dt['cleaned_content'] = dt['cleaned_content'].apply(remove_stopwords)
dt['cleaned_content'].head(12)

def lemmatize_text(tokens):
    return [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens]
dt['cleaned_content'] = dt['cleaned_content'].apply(lemmatize_text)
dt['cleaned_content'].head(12)

def correct_spelling(tokens):
    corrected = []
    for word in tokens:
        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)
        if suggestions:
            corrected.append(suggestions[0].term)
        else:
            corrected.append(word)
    return corrected
dt['cleaned_content'] = dt['cleaned_content'].apply(correct_spelling)
dt['cleaned_content'].head(12)

"""**Ekstrasi Fitur**"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from scipy.special import softmax

model_path = "cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

def Labels_sentiment(text):
    encoded_text = tokenizer(text, return_tensors='pt', truncation=True, padding=True)
    with torch.no_grad():
        output = model(**encoded_text)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)

    scores_dict = {
        'negative': float(scores[0]),
        'neutral': float(scores[1]),
        'positive': float(scores[2])
    }

    label = max(scores_dict, key=scores_dict.get)
    return label, scores_dict

# Untuk menyimpan hasil label dan score
labels = []
scores = []

for text_list in tqdm(dt['cleaned_content']):
    # Join the list of tokens into a string
    text = ' '.join(text_list)
    label, score_dict = Labels_sentiment(text)
    labels.append(label)
    scores.append(score_dict)

# Menambahkan ke DataFrame
dt['label'] = labels
dt['scores_dict'] = scores

dt.info()

dt[['neg', 'neu', 'pos']] = pd.DataFrame(dt['scores_dict'].tolist(), index=df.index)
dt.drop('scores_dict', axis=1, inplace=True)
dt.head(12)

#membuat word cloud
from wordcloud import WordCloud, STOPWORDS
from collections import Counter

#membuat word cloud
from wordcloud import WordCloud, STOPWORDS
from collections import Counter

custom_stopwords = {"tok", "tik", "tiktok","tito", "video", "app", "account", "use", "im","cant","please","didst", "want","even"}

stopwords_set = set(STOPWORDS)
stopwords_set.update(custom_stopwords)

# Join the list of words back into a string for WordCloud
positive_text = ' '.join([' '.join(x) for x in dt[dt['label'] == 'positive']['cleaned_content']])
neutral_text = ' '.join([' '.join(x) for x in dt[dt['label'] == 'neutral']['cleaned_content']])
negative_text = ' '.join([' '.join(x) for x in dt[dt['label'] == 'negative']['cleaned_content']])

# Buat plot horizontal (1 baris, 3 kolom)
fig, axes = plt.subplots(1, 3, figsize=(20, 6))

# WordCloud Positive
wordcloud_pos = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords_set).generate(positive_text)
axes[0].imshow(wordcloud_pos, interpolation='bilinear')
axes[0].axis('off')
axes[0].set_title('Positive')

# WordCloud Neutral
wordcloud_neu = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords_set).generate(neutral_text)
axes[1].imshow(wordcloud_neu, interpolation='bilinear')
axes[1].axis('off')
axes[1].set_title('Neutral')

# WordCloud Negative
wordcloud_neg = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords_set).generate(negative_text)
axes[2].imshow(wordcloud_neg, interpolation='bilinear')
axes[2].axis('off')
axes[2].set_title('Negative')

plt.tight_layout()
plt.show()

fig, axs = plt.subplots(1, 3, figsize=(12, 5))
sns.barplot(data=dt, x='score', y='pos',ax=axs[0])
sns.barplot(data=dt, x='score', y='neu',ax=axs[1])
sns.barplot(data=dt, x='score', y='neg',ax=axs[2])
axs[0].set_title('Positive')
axs[1].set_title('Neutral')
axs[2].set_title('Negative')
plt.tight_layout()
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

# Hitung jumlah masing-masing label
sentiment_counts = dt['label'].value_counts()

# Bar chart
plt.figure(figsize=(6, 4))
sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='pastel')

plt.title('Distribusi Label Sentimen')
plt.xlabel('Sentimen')
plt.ylabel('Jumlah')
plt.tight_layout()
plt.show()

"""**Model Regresi Logistik**


"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.pipeline import FeatureUnion
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.metrics import classification_report

# --- 1. Bagi data jadi Train+Val dan Test dulu ---
X = dt[['cleaned_content', 'score']]
y = dt['label']

X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- 2. Bagi lagi X_temp jadi Train dan Validation ---
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)
# 0.25 dari 80% = 20%, jadi hasil akhir: 60% train, 20% val, 20% test

X_train['cleaned_content'] = X_train['cleaned_content'].apply(lambda x: ' '.join(x))
X_val['cleaned_content'] = X_val['cleaned_content'].apply(lambda x: ' '.join(x))
X_test['cleaned_content'] = X_test['cleaned_content'].apply(lambda x: ' '.join(x))

# --- TF-IDF dan MinMaxScaler ---
tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=1000)
preprocessor = ColumnTransformer(
    transformers=[
        ('tfidf', tfidf, 'cleaned_content'),
        ('scale', MinMaxScaler(), ['score'])
    ])

# --- Pipeline ---
pipeline = Pipeline([
    ('features', preprocessor),
    ('classifier', LogisticRegression(max_iter=1000))
])

# --- Train model ---
pipeline.fit(X_train, y_train)

# --- Validasi (Validation set) ---
y_val_pred = pipeline.predict(X_val)
print("ðŸ“Š Evaluasi regresi logistik pada VALIDATION SET:")
print(classification_report(y_val, y_val_pred))

# --- Tes akhir (Test set) ---
y_test_pred = pipeline.predict(X_test)
print("\nâœ… Evaluasi regresi logistik pada TEST SET:")
print(classification_report(y_test, y_test_pred))

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_test, y_test_pred, labels=pipeline.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipeline.classes_)
disp.plot(cmap="Blues")
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

"""**Model Support Vector Machines**"""

from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay


# Transform the data using the fitted preprocessor
X_train_transformed = preprocessor.transform(X_train)
X_val_transformed = preprocessor.transform(X_val)
X_test_transformed = preprocessor.transform(X_test)

# --- Inisialisasi dan Latih Model SVM ---
svm_model_reused_features = SVC()

# Train the SVM model on the transformed training data
svm_model_reused_features.fit(X_train_transformed, y_train)

# Validasi
y_val_pred_svm_reused = svm_model_reused_features.predict(X_val_transformed)
print("ðŸ“Š Evaluasi SVM (Fitur Reused) pada VALIDATION SET:")
print(classification_report(y_val, y_val_pred_svm_reused))

# --- Tes akhir (Test set) ---
y_test_pred_svm_reused = svm_model_reused_features.predict(X_test_transformed)
print("\nâœ… Evaluasi SVM (Fitur Reused) pada TEST SET:")
print(classification_report(y_test, y_test_pred_svm_reused))

cm_svm_reused = confusion_matrix(y_test, y_test_pred_svm_reused, labels=svm_model_reused_features.classes_)
disp_svm_reused = ConfusionMatrixDisplay(confusion_matrix=cm_svm_reused, display_labels=svm_model_reused_features.classes_)
disp_svm_reused.plot(cmap="Blues")
plt.title("Confusion Matrix - SVM (Reused Features)")
plt.show()

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# Extract metrics from classification reports for Logistic Regression and SVM
logreg_report = classification_report(y_test, y_test_pred, output_dict=True)
svm_report = classification_report(y_test, y_test_pred_svm_reused, output_dict=True)

# Create DataFrames from the reports
logreg_df = pd.DataFrame(logreg_report).transpose()
svm_df = pd.DataFrame(svm_report).transpose()

# Select relevant metrics (precision, recall, f1-score, and accuracy)
metrics = ['precision', 'recall', 'f1-score']
comparison_data = {
    'Metric': [],
    'Logistic Regression': [],
    'SVM': [],
    'Sentiment': []
}

for sentiment in ['negative', 'neutral', 'positive']:
    for metric in metrics:
        comparison_data['Metric'].append(metric)
        comparison_data['Logistic Regression'].append(logreg_df.loc[sentiment, metric])
        comparison_data['SVM'].append(svm_df.loc[sentiment, metric])
        comparison_data['Sentiment'].append(sentiment)

# Add accuracy
comparison_data['Metric'].append('accuracy')
comparison_data['Logistic Regression'].append(logreg_df.loc['accuracy', 'support']) # Accuracy is in the 'support' column for the overall row
comparison_data['SVM'].append(svm_df.loc['accuracy', 'support'])
comparison_data['Sentiment'].append('overall') # Or some indicator for overall accuracy

comparison_df = pd.DataFrame(comparison_data)

# Reshape for plotting
comparison_df_melted = comparison_df.melt(id_vars=['Metric', 'Sentiment'], var_name='Model', value_name='Score')

# Plotting
fig, axes = plt.subplots(1, len(metrics) + 1, figsize=(20, 6), sharey=True) # Add one subplot for accuracy

for i, metric in enumerate(metrics):
    subset = comparison_df_melted[comparison_df_melted['Metric'] == metric]
    sns.barplot(data=subset, x='Sentiment', y='Score', hue='Model', ax=axes[i], palette='viridis')
    axes[i].set_title(f'{metric.capitalize()} Comparison')
    axes[i].set_ylabel(metric.capitalize())
    axes[i].set_xlabel('Sentiment')
    axes[i].set_ylim(0, 1.1) # Increase y-axis limit to make space for text
    axes[i].tick_params(axis='x', rotation=45)
    axes[i].legend(title='Model')

    # Add values on top of bars
    for container in axes[i].containers:
        axes[i].bar_label(container, fmt='%.2f')

# Plot accuracy
accuracy_subset = comparison_df_melted[comparison_df_melted['Metric'] == 'accuracy']
sns.barplot(data=accuracy_subset, x='Sentiment', y='Score', hue='Model', ax=axes[-1], palette='viridis')
axes[-1].set_title('Accuracy Comparison')
axes[-1].set_ylabel('Accuracy')
axes[-1].set_xlabel('')
axes[-1].set_ylim(0, 1.1)
axes[-1].set_xticks([])
axes[-1].legend(title='Model')

# Add values on top of bars for accuracy plot
for container in axes[-1].containers:
    axes[-1].bar_label(container, fmt='%.2f')

plt.tight_layout()
plt.show()

